import { createClient } from '@/lib/supabase/server'
import { type SearchResult } from '@/lib/types/search'

// Keep track of processed results to prevent duplicates within the same stream
const processedResults = new Map<string, Set<string>>()
const processedTimestamps = new Map<string, number>()
const batchQueue = new Map<string, SearchResult[]>()
const processingBatches = new Map<string, NodeJS.Timeout>()

// Clear processed results after 5 minutes
const CLEANUP_INTERVAL = 5 * 60 * 1000 // 5 minutes
setInterval(() => {
  const now = Date.now()
  for (const [key, timestamp] of processedTimestamps.entries()) {
    if (now - timestamp > CLEANUP_INTERVAL) {
      processedResults.delete(key)
      processedTimestamps.delete(key)
      batchQueue.delete(key)
      const timeout = processingBatches.get(key)
      if (timeout) {
        clearTimeout(timeout)
        processingBatches.delete(key)
      }
    }
  }
}, CLEANUP_INTERVAL)

export interface SourceInput {
  id?: string // UUID will be auto-generated by Supabase
  session_id: string
  message_id?: string
  url: string
  title: string
  content: string
  relevance: number
  metadata: Record<string, any>
  created_at?: string // Will be set by Supabase default
}

// Batch size and delay configuration
const BATCH_SIZE = 50 // Increased batch size for better throughput
const BATCH_DELAY = 500 // Reduced delay to process faster
const MAX_QUEUE_SIZE = 1000 // Maximum number of items to queue

async function processBatch(
  sessionId: string,
  messageId: string | undefined,
  searchKey: string
): Promise<SourceInput[]> {
  const currentBatch = batchQueue.get(searchKey) || []
  if (currentBatch.length === 0) return []

  const supabase = await createClient()

  // First verify that the research session exists and belongs to the current user
  const { data: session, error: sessionError } = await supabase
    .from('research_sessions')
    .select('id, user_id')
    .eq('id', sessionId)
    .single()

  if (sessionError || !session) {
    console.error('Session verification failed:', sessionError || 'Session not found')
    return []
  }

  // Get existing sources for this session to avoid duplicates
  const { data: existingSources } = await supabase
    .from('sources')
    .select('url')
    .eq('session_id', sessionId)

  const existingUrls = new Set(existingSources?.map(source => source.url) || [])
  const processedUrls = processedResults.get(searchKey) || new Set()
  
  // Take the next batch of results
  const batchToProcess = currentBatch.splice(0, BATCH_SIZE)
  batchQueue.set(searchKey, currentBatch)
  
  // Deduplicate results based on URL
  const uniqueResults = batchToProcess.reduce((acc: SearchResult[], result) => {
    if (!result.url || 
        existingUrls.has(result.url) || 
        processedUrls.has(result.url)) {
      return acc
    }
    processedUrls.add(result.url)
    acc.push(result)
    return acc
  }, [])

  // Update the processed results tracking
  processedResults.set(searchKey, processedUrls)
  
  if (uniqueResults.length === 0) {
    console.log('No new unique sources in batch')
    return []
  }

  // Map and insert unique results
  const sourceInputs: SourceInput[] = uniqueResults.map((result) => ({
    session_id: sessionId,
    message_id: messageId,
    url: result.url,
    title: result.title || 'Untitled',
    content: result.content || result.snippet || '',
    relevance: result.relevance || 0,
    metadata: {
      source_type: 'web',
      last_updated: new Date().toISOString(),
      language: result.language || 'en',
      word_count: result.wordCount || 0,
      keywords: result.keywords || [],
      fetch_metadata: {
        status_code: result.status || 200,
        content_type: result.contentType || 'text/html',
        encoding: result.encoding || 'utf-8'
      }
    }
  }))

  try {
    const { data, error } = await supabase
      .from('sources')
      .insert(sourceInputs)
      .select()

    if (error) {
      console.error('Error inserting sources:', error)
      return []
    }

    console.log('Batch processed:', {
      searchKey,
      inserted: data.length,
      remaining: currentBatch.length,
      totalProcessed: processedUrls.size
    })

    return data
  } catch (error) {
    console.error('Batch processing error:', error)
    return []
  }
}

export async function saveSearchResultsToSupabase(
  sessionId: string,
  messageId: string | undefined,
  results: SearchResult[]
): Promise<SourceInput[]> {
  if (!sessionId || !results?.length) return []

  const searchKey = `${sessionId}:${messageId || 'direct'}`
  
  // Initialize tracking for this search if not exists
  if (!processedResults.has(searchKey)) {
    processedResults.set(searchKey, new Set())
    processedTimestamps.set(searchKey, Date.now())
    batchQueue.set(searchKey, [])
  }

  // Add new results to the batch queue, respecting max queue size
  const currentBatch = batchQueue.get(searchKey) || []
  const spaceInQueue = MAX_QUEUE_SIZE - currentBatch.length
  
  if (spaceInQueue <= 0) {
    console.log('Queue full, dropping results:', {
      searchKey,
      dropped: results.length
    })
    return []
  }

  const resultsToQueue = results.slice(0, spaceInQueue)
  currentBatch.push(...resultsToQueue)
  batchQueue.set(searchKey, currentBatch)

  console.log('Queue status:', {
    searchKey,
    queued: resultsToQueue.length,
    queueSize: currentBatch.length,
    dropped: results.length - resultsToQueue.length
  })

  // If we're already processing, just return
  if (processingBatches.has(searchKey)) {
    return []
  }

  // Start processing batches
  const processNextBatch = async () => {
    const batch = await processBatch(sessionId, messageId, searchKey)
    
    // Schedule next batch if there are more results
    if (batchQueue.get(searchKey)?.length) {
      const timeout = setTimeout(processNextBatch, BATCH_DELAY)
      processingBatches.set(searchKey, timeout)
    } else {
      processingBatches.delete(searchKey)
    }
    
    return batch
  }

  // Start initial batch processing
  return processNextBatch()
}

export async function getSourcesBySession(sessionId: string) {
  const supabase = await createClient()
  
  const { data, error } = await supabase
    .from('sources')
    .select('*')
    .eq('session_id', sessionId)
    .order('relevance', { ascending: false })

  if (error) {
    console.error('Error fetching sources:', error)
    throw new Error(`Failed to fetch sources: ${error.message}`)
  }

  return data
}

export async function getSourcesByMessage(messageId: string) {
  const supabase = await createClient()
  
  const { data, error } = await supabase
    .from('sources')
    .select('*')
    .eq('message_id', messageId)
    .order('relevance', { ascending: false })

  if (error) {
    console.error('Error fetching sources:', error)
    throw new Error(`Failed to fetch sources: ${error.message}`)
  }

  return data
} 